{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010761,
     "end_time": "2021-02-24T17:36:50.310444",
     "exception": false,
     "start_time": "2021-02-24T17:36:50.299683",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Kaggle Competition : Jane Street Market Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009162,
     "end_time": "2021-02-24T17:36:50.330364",
     "exception": false,
     "start_time": "2021-02-24T17:36:50.321202",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Competition Description**\n",
    "\n",
    "“Buy low, sell high.” It sounds so easy….\n",
    "\n",
    "In reality, trading for profit has always been a difficult problem to solve, even more so in today’s fast-moving and complex financial markets. Electronic trading allows for thousands of transactions to occur within a fraction of a second, resulting in nearly unlimited opportunities to potentially find and take advantage of price differences in real time.\n",
    "In a perfectly efficient market, buyers and sellers would have all the agency and information needed to make rational trading decisions. As a result, products would always remain at their “fair values” and never be undervalued or overpriced. However, financial markets are not perfectly efficient in the real world.\n",
    "Developing trading strategies to identify and take advantage of inefficiencies is challenging. Even if a strategy is profitable now, it may not be in the future, and market volatility makes it impossible to predict the profitability of any given trade with certainty. As a result, it can be hard to distinguish good luck from having made a good trading decision.\n",
    "In the first three months of this challenge, you will build your own quantitative trading model to maximize returns using market data from a major global stock exchange. Next, you’ll test the predictiveness of your models against future market returns and receive feedback on the leaderboard.\n",
    "Your challenge will be to use the historical data, mathematical tools, and technological tools at your disposal to create a model that gets as close to certainty as possible. You will be presented with a number of potential trading opportunities, which your model must choose whether to accept or reject.\n",
    "In general, if one is able to generate a highly predictive model which selects the right trades to execute, they would also be playing an important role in sending the market signals that push prices closer to “fair” values. That is, a better model will mean the market will be more efficient going forward. However, developing good models will be challenging for many reasons, including a very low signal-to-noise ratio, potential redundancy, strong feature correlation, and difficulty of coming up with a proper mathematical formulation.\n",
    "Jane Street has spent decades developing their own trading models and machine learning solutions to identify profitable opportunities and quickly decide whether to execute trades. These models help Jane Street trade thousands of financial products each day across 200 trading venues around the world.\n",
    "Admittedly, this challenge far oversimplifies the depth of the quantitative problems Jane Streeters work on daily, and Jane Street is happy with the performance of its existing trading model for this particular question. However, there’s nothing like a good puzzle, and this challenge will hopefully serve as a fun introduction to a type of data science problem that a Jane Streeter might tackle on a daily basis. Jane Street looks forward to seeing the new and creative approaches the Kaggle community will take to solve this trading challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009206,
     "end_time": "2021-02-24T17:36:50.349038",
     "exception": false,
     "start_time": "2021-02-24T17:36:50.339832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Data Description**\n",
    "\n",
    "This dataset contains an anonymized set of features, feature_{0...129}, representing real stock market data. Each row in the dataset represents a trading opportunity, for which you will be predicting an action value: 1 to make the trade and 0 to pass on it. Each trade has an associated weight and resp, which together represents a return on the trade. The date column is an integer which represents the day of the trade, while ts_id represents a time ordering. In addition to anonymized feature values, you are provided with metadata about the features in features.csv.\n",
    "\n",
    "In the training set, train.csv, you are provided a resp value, as well as several other resp_{1,2,3,4} values that represent returns over different time horizons. These variables are not included in the test set. Trades with weight = 0 were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation.\n",
    "\n",
    "This is a code competition that relies on a time-series API to ensure models do not peek forward in time. To use the API, follow the instructions on the Evaluation page. When you submit your notebook, it will be rerun on an unseen test:\n",
    "\n",
    "    During the model training phase of the competition, this unseen test set is comprised of approximately 1 million rows of historical data.\n",
    "    During the live forecasting phase, the test set will use periodically updated live market data.\n",
    "\n",
    "Note that during the second (forecasting) phase of the competition, the notebook time limits will scale with the number of trades presented in the test set. Refer to the Code Requirements for details.\n",
    "Files\n",
    "\n",
    "    train.csv - the training set, contains historical data and returns\n",
    "    example_test.csv - a mock test set which represents the structure of the unseen test set. You will not be directly using the test set or sample submission in this competition, as the time-series API will get/set the test set and predictions.\n",
    "    example_sample_submission.csv - a mock sample submission file in the correct format\n",
    "    features.csv - metadata pertaining to the anonymized features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009155,
     "end_time": "2021-02-24T17:36:50.367555",
     "exception": false,
     "start_time": "2021-02-24T17:36:50.358400",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Evaluation**\n",
    "\n",
    "This competition is evaluated on a utility score. Each row in the test set represents a trading opportunity for which you will be predicting an action value, 1 to make the trade and 0 to pass on it. Each trade j has an associated weight and resp, which represents a return.\n",
    "\n",
    "For each date i, we define:\n",
    "\n",
    "   $p_i = \\sum_{i=0} (weight_{ij} * resp_{ij} * action_{ij})$,\n",
    "\n",
    "   $t = \\frac{\\sum_{p_i}}{\\sqrt{\\sum_{p_i^2}}} * \\sqrt{\\frac{250}{|i|}}$,\n",
    "\n",
    "                       \n",
    "where $|i|$ is the number of unique dates in the test set. The utility is then defined as:\n",
    "\n",
    "   $u = \\min{(\\max{(t, 0)}, 6)} \\sum{p_i}$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009112,
     "end_time": "2021-02-24T17:36:50.385992",
     "exception": false,
     "start_time": "2021-02-24T17:36:50.376880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " $\\rule{20cm}{0.1pt}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009174,
     "end_time": "2021-02-24T17:36:50.404582",
     "exception": false,
     "start_time": "2021-02-24T17:36:50.395408",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Notebook details\n",
    "\n",
    "In the first part of the model (training only) 29 MLP are trained, where each MLP's inputs are features corresponding to tags described in features.csv. Output for each model is the probability that $\\textit{resp} > 0$.\n",
    "\n",
    "Then, a new (big) MLP is built in the second part (cf. Pytorch -- TagModels + Stacking 2/2) which combined these 29 outputs to a final probability. \n",
    "\n",
    "The objective is to build a global model where features are used in accordance with the way Jane Street has provided metadata --> grouped.\n",
    "\n",
    "$\\underline{\\textit{NB :}}$\n",
    "- $\\textit{feature_0}$ is added to each tag.\n",
    "- Data whith $\\textit{date} < 85$ are deleted (cf. https://www.kaggle.com/c/jane-street-market-prediction/discussion/201930)\n",
    "- Data whith $\\textit{weight} = 0$ are deleted\n",
    "- A customized loss is built manually in order to maximize the first part of the $\\textit{utility}$ evaluation function (Kaggle competition metric) : $1 - \\frac{\\sum_{i} {p(pred)_i}}{\\sum_{i} {p_i}}$\n",
    "\n",
    "    where :\n",
    "    \n",
    "    $p(pred)_i = \\sum_{j=0} (weight_{ij} * resp_{ij} * action(pred)_{ij})$,\n",
    "\n",
    "    $p_i = \\sum_{j=0} (weight_{ij} * resp_{ij} * action_{ij})$ and\n",
    "\n",
    "    $action_{ij} = \\mathbb{1}_{\\{resp_{ij} > 0\\}}$\n",
    "            \n",
    "            \n",
    "- Early stopping is used($\\searrow$ overfitting).\n",
    "- Train set used for training TagModel1 is different than train set used for training TagModel2 etc ($\\searrow$ overfitting).\n",
    "- Time ordering is partialy or will be broken by : \n",
    "    - the deletion of data with $\\textit{weight = 0}$.\n",
    "    - the gap between train set and eval set (Private LeaderBoard) ?\n",
    "    - the delay between trades is not equal ?\n",
    "    - the fact that the model is not a RNN.\n",
    "    \n",
    "    So, I decided to fully break time ordering by shuffling data before training. It seems to have the effect of reducing overfitting too.\n",
    "\n",
    "- shuffling involves that $|i|$ on validation is inconsistent so I added an option into utility function for fixing $\\sqrt{\\frac{250}{|i|}} = 1$.\n",
    "    \n",
    "- $resp_1, resp_2, resp_3, resp_4$ are not used but it could be a good idea to integrate them.\n",
    "\n",
    "- Warning GPU Quota : this notebook runs in 9 hours on my computer so weights for each TagModel are available in input\\weights-tagmodels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009237,
     "end_time": "2021-02-24T17:36:50.423172",
     "exception": false,
     "start_time": "2021-02-24T17:36:50.413935",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Load Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-24T17:36:50.451032Z",
     "iopub.status.busy": "2021-02-24T17:36:50.450353Z",
     "iopub.status.idle": "2021-02-24T17:36:53.013851Z",
     "shell.execute_reply": "2021-02-24T17:36:53.013266Z"
    },
    "papermill": {
     "duration": 2.579469,
     "end_time": "2021-02-24T17:36:53.014070",
     "exception": false,
     "start_time": "2021-02-24T17:36:50.434601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version : 1.7.0\n",
      "GPU : Tesla P100-PCIE-16GB available\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datatable as dt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from scipy.special import expit\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(f\"Pytorch version : {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU : {torch.cuda.get_device_name()} available\")\n",
    "else:\n",
    "    print(\"Error : GPU not available\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010381,
     "end_time": "2021-02-24T17:36:53.036236",
     "exception": false,
     "start_time": "2021-02-24T17:36:53.025855",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-24T17:36:53.069671Z",
     "iopub.status.busy": "2021-02-24T17:36:53.067942Z",
     "iopub.status.idle": "2021-02-24T17:36:53.070318Z",
     "shell.execute_reply": "2021-02-24T17:36:53.070710Z"
    },
    "papermill": {
     "duration": 0.02439,
     "end_time": "2021-02-24T17:36:53.070838",
     "exception": false,
     "start_time": "2021-02-24T17:36:53.046448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    return None\n",
    "\n",
    "def utility(dates, weights, true_resp, actions, use_mult=False):\n",
    "    \"\"\"Jane Street evaluation metric\"\"\"\n",
    "    Pi = weights * true_resp * actions\n",
    "    if use_mult:\n",
    "        mult = np.sqrt(250 / np.bincount(dates).shape[0])\n",
    "    else:\n",
    "        mult = 1\n",
    "    sum_Pi = Pi.sum() \n",
    "    sum_pi_squared = np.sqrt((Pi ** 2).sum())\n",
    "    t = (sum_Pi / sum_pi_squared) * mult\n",
    "    u = min(max(t, 0), 6) * sum_Pi\n",
    "    return u\n",
    "\n",
    "def compute_utility_many(predictions, dates, weights, true_resp, interval=np.linspace(0, 1, 101)):\n",
    "    \"\"\"given predictions probability compute utility for many threshold\"\"\"\n",
    "    lst_ut = []\n",
    "    for v in interval:\n",
    "        actions =  (predictions > v).astype(int)\n",
    "        ut = utility(dates=dates, weights=weights, true_resp=true_resp, actions=actions)\n",
    "        lst_ut.append((v, ut))\n",
    "    return lst_ut\n",
    "\n",
    "def build_dic_records(datasets=[], dic_tags=None, records=[]):\n",
    "    \"\"\"save training data into python dict\"\"\"\n",
    "    dic = {}\n",
    "    for dataset in datasets:\n",
    "        dic[dataset] = {}\n",
    "        for tag in range(len(dic_tags)):\n",
    "            dic[dataset]['tag_'+str(tag)] = {}\n",
    "            for record in records:\n",
    "                dic[dataset]['tag_'+str(tag)][record] = []\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010061,
     "end_time": "2021-02-24T17:36:53.091664",
     "exception": false,
     "start_time": "2021-02-24T17:36:53.081603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-24T17:36:53.120600Z",
     "iopub.status.busy": "2021-02-24T17:36:53.119929Z",
     "iopub.status.idle": "2021-02-24T17:36:53.122341Z",
     "shell.execute_reply": "2021-02-24T17:36:53.122706Z"
    },
    "papermill": {
     "duration": 0.020005,
     "end_time": "2021-02-24T17:36:53.122842",
     "exception": false,
     "start_time": "2021-02-24T17:36:53.102837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SEED = 2021\n",
    "DATE_NOW = datetime.now().__format__(\"%Y-%m-%d_%H:%M:%S\")\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "\n",
    "PATH_ROOT = \"/kaggle/input/jane-street-market-prediction\"\n",
    "PATH_WEIGHT_TAGMODEL = \"../input/weights-tagmodels\"\n",
    "PATH_DATA = os.path.join(PATH_ROOT, \"train.csv\")\n",
    "PATH_FEATURES =  os.path.join(PATH_ROOT, \"features.csv\")\n",
    "\n",
    "LST_FEATURES = [\"feature_\"+str(n_feat) for n_feat in range(0, 130, 1)]\n",
    "LST_TARGETS = [\"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\"]\n",
    "\n",
    "SIZE_TRAIN = .85\n",
    "TRAINING_TAGS = False\n",
    "SAVE_DICT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010163,
     "end_time": "2021-02-24T17:36:53.143294",
     "exception": false,
     "start_time": "2021-02-24T17:36:53.133131",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-24T17:36:53.170668Z",
     "iopub.status.busy": "2021-02-24T17:36:53.170016Z",
     "iopub.status.idle": "2021-02-24T17:37:59.196613Z",
     "shell.execute_reply": "2021-02-24T17:37:59.196175Z"
    },
    "papermill": {
     "duration": 66.043113,
     "end_time": "2021-02-24T17:37:59.196745",
     "exception": false,
     "start_time": "2021-02-24T17:36:53.153632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape : (1862597, 138)\n",
      "CPU times: user 28.6 s, sys: 9.13 s, total: 37.8 s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "seed_everything(seed=SEED)\n",
    "\n",
    "data = dt.fread(PATH_DATA).to_pandas()  # fast loading\n",
    "data = data[data.date > 85]  # delete date < 85\n",
    "data = data.sample(frac=1)  # shuffle\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(f\"shape : {data.shape}\")\n",
    "\n",
    "df_tags = pd.read_csv(PATH_FEATURES, index_col=\"feature\")\n",
    "dic_tags = {}\n",
    "for n, tag in enumerate(df_tags.columns):\n",
    "    lst_features = df_tags[tag][df_tags[tag] == True].index.tolist()\n",
    "    lst_num_features = [int(e.strip(\"feature_\")) for e in lst_features]\n",
    "    dic_tags[str(n)] = lst_num_features\n",
    "\n",
    "# add feature_0 in all tags\n",
    "for e in dic_tags.keys():\n",
    "    dic_tags[e].append(0)\n",
    "    \n",
    "f_mean = data[LST_FEATURES[1:]].mean()\n",
    "data = data.loc[data.weight > 0].reset_index(drop = True)  # delete 0 weight data\n",
    "data[LST_FEATURES[1:]] = data[LST_FEATURES[1:]].fillna(f_mean)  # filling NaN by mean of each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010848,
     "end_time": "2021-02-24T17:37:59.218919",
     "exception": false,
     "start_time": "2021-02-24T17:37:59.208071",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Pytorch utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-24T17:37:59.265251Z",
     "iopub.status.busy": "2021-02-24T17:37:59.259108Z",
     "iopub.status.idle": "2021-02-24T17:37:59.267614Z",
     "shell.execute_reply": "2021-02-24T17:37:59.267204Z"
    },
    "papermill": {
     "duration": 0.037831,
     "end_time": "2021-02-24T17:37:59.267721",
     "exception": false,
     "start_time": "2021-02-24T17:37:59.229890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \n",
    "    def __init__(self, patience=7, mode=\"max\", delta=0.0, verbose=False, trace_func=print, path=\"checkpoint.pt\"):\n",
    "        \n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.mode = mode\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        self.verbose = verbose\n",
    "        self.trace_func = trace_func\n",
    "        self.path = path\n",
    "        \n",
    "        if self.mode == \"min\":\n",
    "            self.val_score = np.Inf\n",
    "            \n",
    "        else:\n",
    "            self.val_score = -np.Inf\n",
    "\n",
    "    def __call__(self, epoch_score, model):\n",
    "\n",
    "        if self.mode == \"min\":\n",
    "            score = -1.0 * epoch_score\n",
    "            \n",
    "        elif self.mode == \"max\":\n",
    "            score = np.copy(epoch_score)\n",
    "        \n",
    "        # first epoch\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(epoch_score, model)\n",
    "        \n",
    "        # best score NOT modified\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                \n",
    "        # best score modified        \n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(epoch_score, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, epoch_score, model):\n",
    "        \"\"\"\n",
    "        Save model when validation loss decrease.\n",
    "        \"\"\"\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation metric moving : ({self.val_score:.6f} --> {epoch_score:.6f}).  Saving model ...')\n",
    "            \n",
    "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "            \n",
    "        self.val_score = epoch_score\n",
    "\n",
    "class BuildDataset:\n",
    "    \n",
    "    def __init__(self, df, col_x, target):\n",
    "        self.col_x = col_x.copy()\n",
    "        self.target = target.copy()\n",
    "        self.X = df[self.col_x].values\n",
    "        self.y = (df[self.target] > 0).astype('int').values\n",
    "        self.weights = df.weight.values\n",
    "        self.resps = df.resp.values\n",
    "        self.actions = (df.resp > 0 ).astype('int').values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': torch.tensor(self.X[idx], dtype=torch.float),\n",
    "            'label': torch.tensor(self.y[idx], dtype=torch.float),\n",
    "            'weights': torch.tensor(self.weights[idx], dtype=torch.float),\n",
    "            'resps': torch.tensor(self.resps[idx], dtype=torch.float),\n",
    "            'actions': torch.tensor(self.actions[idx], dtype=torch.float)\n",
    "        }\n",
    "    \n",
    "class P1UtilityLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    customized loss based on first part of utility evaluation metric\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, threshold=.5):\n",
    "        super(P1UtilityLoss, self).__init__()\n",
    "        self.threshold = torch.tensor(threshold)\n",
    "        \n",
    "    def forward(self, true_actions, pred, weights, resps):\n",
    "        w_r = torch.mul(weights, resps)\n",
    "        pi_true = torch.mul(w_r, true_actions)\n",
    "        pi_pred = torch.mul(w_r, pred)\n",
    "        pi_true_sum = torch.sum(pi_true)\n",
    "        pi_pred_sum = torch.sum(pi_pred)\n",
    "        res = torch.sub(torch.tensor(1), torch.div(pi_pred_sum, pi_true_sum))\n",
    "        return res\n",
    "\n",
    "class TagModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    MLP : batchnorm0 > dropout > dense0 > relu > batchnorm1 > dropout > dense1 > sigmoid\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dic_tags, tag_number, input_size, output_size, threshold=.5, rate_dropout=.1):\n",
    "        \n",
    "        super(TagModel, self).__init__()\n",
    "        self.dic_tags = dic_tags\n",
    "        self.tag_number = str(tag_number)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.rate_dropout = rate_dropout\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(self.rate_dropout)\n",
    "        self.batch_norm0 = nn.BatchNorm1d(len(self.dic_tags[self.tag_number]))\n",
    "        self.batch_norm1 = nn.BatchNorm1d(2 * len(self.dic_tags[self.tag_number]))\n",
    "        self.dense0 = torch.nn.Linear(\n",
    "            len(self.dic_tags[self.tag_number]),\n",
    "            2 * len(self.dic_tags[self.tag_number])\n",
    "        )\n",
    "        self.dense1 = torch.nn.Linear(\n",
    "            2 * len(self.dic_tags[self.tag_number]),\n",
    "            self.output_size\n",
    "        )\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.batch_norm0(x[:, self.dic_tags[self.tag_number]])\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense0(x)\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense1(x)\n",
    "        \n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010927,
     "end_time": "2021-02-24T17:37:59.289813",
     "exception": false,
     "start_time": "2021-02-24T17:37:59.278886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Training all TagModel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-24T17:37:59.330810Z",
     "iopub.status.busy": "2021-02-24T17:37:59.314991Z",
     "iopub.status.idle": "2021-02-24T17:37:59.348615Z",
     "shell.execute_reply": "2021-02-24T17:37:59.348972Z"
    },
    "papermill": {
     "duration": 0.048225,
     "end_time": "2021-02-24T17:37:59.349104",
     "exception": false,
     "start_time": "2021-02-24T17:37:59.300879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model : architecture\n",
    "batch_size = 16384\n",
    "input_size = 130\n",
    "output_size = 1\n",
    "\n",
    "# Model : training\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "threshold =.5\n",
    "es_mode = \"min\"\n",
    "patience = 3\n",
    "\n",
    "dic_records = build_dic_records(\n",
    "            datasets=[\"train\", \"val\"],\n",
    "            dic_tags=dic_tags,\n",
    "            records=[\"lst_loss_epoch\", \"lst_loss_batch\", \"lst_utility\", \"lst_accuracy\", \"lst_precision\", \"lst_recall\"]\n",
    ")\n",
    "\n",
    "if TRAINING_TAGS:\n",
    "\n",
    "    pbar_tag = tqdm(total=len(dic_tags), position=0)\n",
    "\n",
    "    for tag in range(len(dic_tags)):\n",
    "        str_tag = str(tag)\n",
    "\n",
    "        # sample data\n",
    "        data = data.sample(frac=1)\n",
    "        data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        train_index = [e for e in range(0, int(data.shape[0] * SIZE_TRAIN), 1)]\n",
    "        val_index = [e for e in range(max(train_index), data.shape[0], 1)]\n",
    "\n",
    "        # build sets for training\n",
    "        train = data.loc[train_index]\n",
    "        val = data.loc[val_index]\n",
    "        train_set = BuildDataset(df=train.loc[train_index], col_x=LST_FEATURES, target=LST_TARGETS)\n",
    "        val_set = BuildDataset(df=val.loc[val_index], col_x=LST_FEATURES, target=LST_TARGETS)\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "        val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size)\n",
    "\n",
    "        # compute and save perfect utility for each set\n",
    "        perfect_train_utility = utility(dates=train.date.values, weights=train.weight.values, true_resp=train.resp.values, actions=(train.resp > 0).astype(int))\n",
    "        perfect_val_utility = utility(dates=val.date.values, weights=val.weight.values, true_resp=val.resp.values, actions=(val.resp > 0).astype(int))\n",
    "        dic_records[\"train\"][\"tag_\"+str_tag][\"perfect_utility\"] = perfect_train_utility\n",
    "        dic_records[\"val\"][\"tag_\"+str_tag][\"perfect_utility\"] = perfect_val_utility\n",
    "        dic_records[\"train\"][\"tag_\"+str_tag][\"lst_index\"] = train_index\n",
    "        dic_records[\"val\"][\"tag_\"+str_tag][\"lst_index\"] = val_index\n",
    "\n",
    "        # define utils for model\n",
    "        torch.cuda.empty_cache()\n",
    "        model = TagModel(dic_tags, tag, input_size, output_size).to(DEVICE)\n",
    "        p1_utility_loss = P1UtilityLoss(threshold)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        path_model = f\"./TagModel{str_tag}_{DATE_NOW}.pt\"\n",
    "        early_stopping = EarlyStopping(mode=es_mode, patience=patience, verbose=True, path=path_model)\n",
    "\n",
    "        pbar_epoch = tqdm(total=num_epochs, position=1)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            dic_records[\"train\"][\"tag_\"+str_tag][\"lst_loss_batch\"] = []\n",
    "            dic_records[\"val\"][\"tag_\"+str_tag][\"lst_loss_batch\"] = []\n",
    "            train_outputs = np.empty(shape=(len(train)))\n",
    "            val_outputs = np.empty(shape=(len(val)))\n",
    "\n",
    "            pbar_batch = tqdm(total=len(train_loader), position=2)\n",
    "\n",
    "            for i, train_batch in enumerate(train_loader):\n",
    "\n",
    "                # Allow training <=> moving weights\n",
    "                model.train()\n",
    "\n",
    "                # Remember index on train data\n",
    "                start_ind = i * batch_size\n",
    "                end_ind = start_ind + batch_size\n",
    "\n",
    "                # Send to GPU\n",
    "                X = train_batch[\"features\"].to(DEVICE)\n",
    "                y = train_batch[\"label\"].to(DEVICE)\n",
    "                w = train_batch[\"weights\"].to(DEVICE)\n",
    "                r = train_batch[\"resps\"].to(DEVICE)\n",
    "                a = train_batch[\"actions\"].to(DEVICE)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(X.float())\n",
    "                loss = p1_utility_loss(a.to(DEVICE), outputs[:,0].to(DEVICE), w.to(DEVICE), r.to(DEVICE))\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad() \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Save \n",
    "                dic_records[\"train\"][\"tag_\"+str_tag][\"lst_loss_batch\"].append(loss.item())\n",
    "                train_outputs[start_ind:end_ind] = outputs[:,0].cpu().detach().numpy()\n",
    "                pbar_batch.update(1)\n",
    "\n",
    "            # compute loss on validation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for i_, val_batch in enumerate(val_loader):\n",
    "\n",
    "                    # Remember index on validation data\n",
    "                    start_ind_ = i_ * batch_size\n",
    "                    end_ind_ = start_ind_ + batch_size\n",
    "\n",
    "                    # Send to GPU\n",
    "                    X_ = val_batch[\"features\"].to(DEVICE)\n",
    "                    y_ = val_batch[\"label\"].to(DEVICE)\n",
    "                    w_ = val_batch[\"weights\"].to(DEVICE)\n",
    "                    r_ = val_batch[\"resps\"].to(DEVICE)\n",
    "                    a_ = val_batch[\"actions\"].to(DEVICE)\n",
    "\n",
    "                    # Compute\n",
    "                    outputs_ = model(X_.float())\n",
    "                    loss_ = p1_utility_loss(a_.to(DEVICE), outputs_[:,0].to(DEVICE), w_.to(DEVICE), r_.to(DEVICE))\n",
    "                    # Save\n",
    "                    dic_records[\"val\"][\"tag_\"+str_tag][\"lst_loss_batch\"].append(loss_.item())\n",
    "                    val_outputs[start_ind_:end_ind_] = outputs_[:,0].cpu().detach().numpy()\n",
    "            dic_records[\"train\"][\"tag_\"+str_tag][\"lst_loss_epoch\"].append(np.mean(dic_records[\"train\"][\"tag_\"+str_tag][\"lst_loss_batch\"]))\n",
    "            dic_records[\"val\"][\"tag_\"+str_tag][\"lst_loss_epoch\"].append(np.mean(dic_records[\"val\"][\"tag_\"+str_tag][\"lst_loss_batch\"]))\n",
    "\n",
    "            # Early Stopping on loss function\n",
    "            train_actions = (train_outputs > threshold).astype(int)\n",
    "            train_utility = utility(dates=train.date.values, weights=train.weight.values, true_resp=train.resp.values, actions=train_actions)\n",
    "            train_accuracy = accuracy_score((train.resp > 0).astype(int), train_actions)\n",
    "            train_precision = precision_score((train.resp > 0).astype(int), train_actions)\n",
    "            train_recall = recall_score((train.resp > 0).astype(int), train_actions)\n",
    "            dic_records[\"train\"][\"tag_\"+str_tag][\"lst_utility\"].append(train_utility)\n",
    "            dic_records[\"train\"][\"tag_\"+str_tag][\"lst_accuracy\"].append(train_accuracy)\n",
    "            dic_records[\"train\"][\"tag_\"+str_tag][\"lst_precision\"].append(train_precision)\n",
    "            dic_records[\"train\"][\"tag_\"+str_tag][\"lst_recall\"].append(train_recall)\n",
    "            \n",
    "            val_actions = (val_outputs > threshold).astype(int)\n",
    "            val_utility = utility(dates=val.date.values, weights=val.weight.values, true_resp=val.resp.values, actions=val_actions)\n",
    "            val_accuracy = accuracy_score((val.resp > 0).astype(int), val_actions)\n",
    "            val_precision = precision_score((val.resp > 0).astype(int), val_actions)\n",
    "            val_recall = recall_score((val.resp > 0).astype(int), val_actions)\n",
    "            dic_records[\"val\"][\"tag_\"+str_tag][\"lst_utility\"].append(val_utility)\n",
    "            dic_records[\"val\"][\"tag_\"+str_tag][\"lst_accuracy\"].append(val_accuracy)\n",
    "            dic_records[\"val\"][\"tag_\"+str_tag][\"lst_precision\"].append(val_precision)\n",
    "            dic_records[\"val\"][\"tag_\"+str_tag][\"lst_recall\"].append(val_recall)\n",
    "            \n",
    "            msg1 = '~~~ Epoch [{}/{}], Loss train: {:.4f}, Loss val {:.4f}'.format(\n",
    "                    epoch + 1,\n",
    "                    num_epochs,\n",
    "                    dic_records[\"train\"][\"tag_\"+str_tag][\"lst_loss_epoch\"][-1],\n",
    "                    dic_records[\"val\"][\"tag_\"+str_tag][\"lst_loss_epoch\"][-1]\n",
    "            )\n",
    "            msg2 = '~~~ train utility: {:.4f}, val utility {:.4f}'.format(\n",
    "                    dic_records[\"train\"][\"tag_\"+str_tag][\"lst_utility\"][-1],\n",
    "                    dic_records[\"val\"][\"tag_\"+str_tag][\"lst_utility\"][-1]\n",
    "            )\n",
    "            pbar_epoch.update(1)\n",
    "            pbar_epoch.write(msg1)\n",
    "            pbar_epoch.write(msg2)\n",
    "\n",
    "            early_stopping(dic_records[\"val\"][\"tag_\"+str_tag][\"lst_loss_epoch\"][-1], model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "                \n",
    "        pbar_tag.update(1)\n",
    "        pbar_tag.write(f\"End training TagModel {str_tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011248,
     "end_time": "2021-02-24T17:37:59.371601",
     "exception": false,
     "start_time": "2021-02-24T17:37:59.360353",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Save dict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-24T17:37:59.398087Z",
     "iopub.status.busy": "2021-02-24T17:37:59.397438Z",
     "iopub.status.idle": "2021-02-24T17:37:59.400321Z",
     "shell.execute_reply": "2021-02-24T17:37:59.399913Z"
    },
    "papermill": {
     "duration": 0.017597,
     "end_time": "2021-02-24T17:37:59.400428",
     "exception": false,
     "start_time": "2021-02-24T17:37:59.382831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SAVE_DICT:\n",
    "    with open(\"./dic_records_training_TagModel.json\", \"w\") as out:  \n",
    "        json.dump(dic_records, out) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 75.024164,
   "end_time": "2021-02-24T17:38:00.420424",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-02-24T17:36:45.396260",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
